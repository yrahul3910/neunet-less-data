{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.layers import BatchNormalization, Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ryedida/Desktop/menzies/dnn-less-data/code\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path\n",
    "base_path = '../../data/Change-Level-Prediction-Data-20191107T052353Z-001/Change-Level-Prediction-Data/ICSE-2016-PROMISE DefectData/'\n",
    "train_data_fname = 'ant-1.5.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "train_df = pd.read_csv(base_path + train_data_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for deep learners\n",
    "batch_size = 128\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X and y\n",
    "x_train = train_df.drop('bug', axis=1)\n",
    "x_train.drop(['name', 'version', 'name.1'], axis=1, inplace=True)\n",
    "y_train = train_df['bug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "293\n"
     ]
    }
   ],
   "source": [
    "idx = np.where(y_train > 1)[0]\n",
    "print(len(idx))\n",
    "print(len(x_train))\n",
    "x_train.drop(idx, axis=0, inplace=True)\n",
    "y_train.drop(idx, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parabola(x):\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(units=2, n_layers=1, deep_learner=False, batchnorm=False, activation='relu'):\n",
    "    \"\"\"\n",
    "    Returns a model.\n",
    "    \n",
    "    Params:\n",
    "    =======\n",
    "    units: int - Number of input units. Only for deep learners.\n",
    "    layers: int - Number of hidden layers. Only for deep learners.\n",
    "    deep_learner: boolean - To use deep learner, set to True.\n",
    "    batchnorm: boolean - If True, uses Batch Normalization.\n",
    "    \n",
    "    Returns:\n",
    "    ========\n",
    "    An instance of keras.Sequential or AdaBoost from sklearn.\n",
    "    \"\"\"\n",
    "    if deep_learner:\n",
    "        layers = []\n",
    "        \n",
    "        # Add first hidden layer set.\n",
    "        first = Dense(units, input_shape=(units,))\n",
    "        layers.append(first)\n",
    "        \n",
    "        if batchnorm:\n",
    "            layers.append(BatchNormalization())\n",
    "        \n",
    "        layers.append(Activation(activation))\n",
    "        \n",
    "        # Add the other hidden layers.\n",
    "        for i in range(n_layers - 1):\n",
    "            layers.append(Dense(units))\n",
    "            \n",
    "            if batchnorm:\n",
    "                layers.append(BatchNormalization())\n",
    "            \n",
    "            layers.append(Activation(activation))\n",
    "        \n",
    "        # Add the output layer.\n",
    "        layers.append(Dense(1))\n",
    "        layers.append(Activation('sigmoid'))\n",
    "        \n",
    "        return Sequential(layers)\n",
    "    else:\n",
    "        return AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(x, y, n_desired):\n",
    "    \"\"\"\n",
    "    Picks n_desired samples from x and y, trying to maintain a balanced dataset\n",
    "    and oversampling when necessary.\n",
    "    \n",
    "    Args:\n",
    "    =====\n",
    "    x, y: dataset\n",
    "    n_desired: int\n",
    "    \"\"\"\n",
    "    m = len(x)\n",
    "    \n",
    "    # We need to pick n_desired random samples, trying to maintain a\n",
    "    # balanced dataset\n",
    "    c0_idx = np.where(y == 0)[0]\n",
    "    c1_idx = np.where(y == 1)[0]\n",
    "        \n",
    "    c0_samples = np.random.choice(c0_idx, np.floor(n_desired / 2))\n",
    "    c1_samples = np.random.choice(c1_idx, np.ceil(n_desired / 2))\n",
    "        \n",
    "    pick_idx = np.concatenate((c0_samples, c1_samples))\n",
    "    return x[pick_idx], y[pick_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_experiment(x_resampled, y_resampled, x_test, y_test, deep_learner=False, n_layers=1, batchnorm=False, lipschitz_lr=False, \n",
    "                       reduce_data=False, embedding_dims=2, k=7, oversample=False, oversampler=None,\n",
    "                       activation='relu'):\n",
    "    \"\"\"\n",
    "    Runs one experiment.\n",
    "    \n",
    "    Params:\n",
    "    =======\n",
    "    x, y: np.array - Input data\n",
    "    deep_learner: boolean - If True, uses a deep learner.\n",
    "    layers: int - Number of hidden layers. Deep learners only.\n",
    "    batchnorm: boolean - If True, uses Batch Normalization in deep learner.\n",
    "    lipschitz_lr: boolean - If True, uses LipschitzLR\n",
    "    reduce_data: boolean - If True, reduces data using Ivis.\n",
    "    k: int - k used in Ivis reduction.\n",
    "    embedding_dims: int - Number of dimensions to reduce data to\n",
    "    oversample: boolean - If True, oversamples dataset.\n",
    "    oversampler: str or None - One of None, 'smote', and 'random'.\n",
    "    \n",
    "    Returns:\n",
    "    ========\n",
    "    prec, recall, f1, time - Precision, recall, and F-1 scores of the model, along with runtime\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Step 3: Get the model\n",
    "    model = get_model(units=x_resampled.shape[1],n_layers=n_layers, \n",
    "                      deep_learner=deep_learner, batchnorm=batchnorm,\n",
    "                      activation=activation)\n",
    "    \n",
    "    # Step 4: Train model\n",
    "    if deep_learner:\n",
    "        ####################\n",
    "        # LipschitzLR code #\n",
    "        ####################    \n",
    "        def lr_schedule(epoch):\n",
    "            \"\"\"Learning Rate Schedule\n",
    "            # Arguments\n",
    "                epoch (int): The number of epochs\n",
    "            # Returns\n",
    "                lr (float32): learning rate\n",
    "            \"\"\"\n",
    "\n",
    "            Kz = 0.\n",
    "            for i in range((len(x_resampled) - 1) // batch_size + 1):\n",
    "                start_i = i * batch_size\n",
    "                end_i = start_i + batch_size\n",
    "                xb = x_resampled[start_i:end_i]\n",
    "\n",
    "                activ = np.linalg.norm(func([xb]))\n",
    "                if activ > Kz:\n",
    "                    Kz = activ\n",
    "\n",
    "            K_ = ((num_classes - 1) * Kz) / (num_classes * batch_size)\n",
    "            lr = 1 / K_\n",
    "            #print('Epoch', epoch + 1, 'LR =', lr)\n",
    "            return lr\n",
    "        ####################\n",
    "        \n",
    "        if lipschitz_lr:\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='sgd')\n",
    "            func = K.function([model.layers[0].input], [model.layers[-2].output])\n",
    "            lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "            \n",
    "            start = time.time()\n",
    "            model.fit(x_resampled, y_resampled, batch_size=batch_size, verbose=0, \n",
    "                      validation_data=(x_test, y_test), epochs=100, callbacks=[lr_scheduler])\n",
    "            end = time.time()\n",
    "        else:\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam')\n",
    "            \n",
    "            start = time.time()\n",
    "            model.fit(x_resampled, y_resampled, batch_size=batch_size, verbose=0, \n",
    "                      validation_data=(x_test, y_test), epochs=100)\n",
    "            end = time.time()\n",
    "        \n",
    "    else:\n",
    "        start = time.time()\n",
    "        model.fit(x_resampled, y_resampled)\n",
    "        end = time.time()\n",
    "    \n",
    "    # Step 5: Evaluate model\n",
    "    evaluators = [precision_score, recall_score, f1_score]\n",
    "    if deep_learner:\n",
    "        prec, recall, f1 = [f(y_test, model.predict_classes(x_test)) for f in evaluators]\n",
    "    else:\n",
    "        prec, recall, f1 = [f(y_test, model.predict(x_test)) for f in evaluators]\n",
    "    \n",
    "    return prec, recall, f1, (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_20_experiments(reduce_data=False, n_desired=100, \n",
    "                       embedding_dims=2, k=7, lipschitz_lr=False, activation='relu', **kwargs):\n",
    "    global x_train, y_train, x_test, y_test\n",
    "    prec = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    times = []\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    # Step 1: Reduce dimensions --> (x_*_reduced, y_*)\n",
    "    x_train_reduced = np.array(x_train)\n",
    "    x_test_reduced = np.array(x_test)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    if reduce_data:\n",
    "        ivis = Ivis(embedding_dims=embedding_dims, k=k, verbose=0)\n",
    "        x_train_reduced = ivis.fit_transform(x_train_reduced)\n",
    "        x_test_reduced = ivis.transform(x_test_reduced)\n",
    "\n",
    "    # Step 2: Oversample data --> (x_resampled, y_resampled)\n",
    "    x_resampled, y_resampled = resample(x_train_reduced, y_train, n_desired)\n",
    "\n",
    "    while count != 20:\n",
    "        try:          \n",
    "            precision, rec, f1_, time_ = run_one_experiment(x_resampled, y_resampled,\n",
    "                                                            x_test_reduced, y_test, \n",
    "                                                            lipschitz_lr=lipschitz_lr, \n",
    "                                                            activation=activation, **kwargs)\n",
    "            prec.append(precision)\n",
    "            recall.append(rec)\n",
    "            f1.append(f1_)\n",
    "            times.append(time_)\n",
    "            count += 1\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            print('Retrying...')\n",
    "            if lipschitz_lr:\n",
    "                pass\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    print('Precision:', prec, '\\nRecall:', recall, '\\nF1:', f1, '\\nRuntime:', times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experimental_config():\n",
    "    configs = [\n",
    "        {'deep_learner': True, 'n_layers': '1', 'lipschitz_lr': False, 'reduce_data': True, 'embedding_dims': '2', 'k': '4', 'oversample': True, 'activation': 'relu'},\n",
    "        {'deep_learner': True, 'n_layers': '1', 'lipschitz_lr': False, 'reduce_data': False, 'embedding_dims': '2', 'k': '3', 'oversample': True, 'activation': 'parabola'},\n",
    "        {'deep_learner': True, 'n_layers': '1', 'lipschitz_lr': False, 'reduce_data': True, 'embedding_dims': '2', 'k': '3', 'oversample': True, 'activation': 'relu'},\n",
    "        {'deep_learner': True, 'n_layers': '1', 'lipschitz_lr': False, 'reduce_data': True, 'embedding_dims': '3', 'k': '3', 'oversample': True, 'activation': 'relu'},\n",
    "        {'deep_learner': True, 'n_layers': '1', 'lipschitz_lr': False, 'reduce_data': True, 'embedding_dims': '2', 'k': '5', 'oversample': True, 'activation': 'relu'},\n",
    "        {'deep_learner': True, 'n_layers': '1', 'lipschitz_lr': False, 'reduce_data': True, 'embedding_dims': '3', 'k': '5', 'oversample': True, 'activation': 'relu'},\n",
    "        {'deep_learner': True, 'n_layers': '1', 'lipschitz_lr': False, 'reduce_data': True, 'embedding_dims': '3', 'k': '4', 'oversample': True, 'activation': 'relu'},\n",
    "        {'deep_learner': True, 'n_layers': '1', 'lipschitz_lr': False, 'reduce_data': False, 'embedding_dims': '2', 'k': '3', 'oversample': True, 'activation': 'relu'},\n",
    "        {'deep_learner': True, 'n_layers': '1', 'lipschitz_lr': False, 'reduce_data': False, 'embedding_dims': '2', 'k': '3', 'oversample': False, 'activation': 'parabola'}\n",
    "    ]\n",
    "    \n",
    "    complete = []\n",
    "\n",
    "    for config in configs:\n",
    "        if config not in complete:\n",
    "            yield config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for _ in get_experimental_config():\n",
    "    count += 1\n",
    "print(count * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, config in enumerate(get_experimental_config()):\n",
    "    print('Experiment', i + 1, '\\b:', config)\n",
    "    run_20_experiments(**config)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
